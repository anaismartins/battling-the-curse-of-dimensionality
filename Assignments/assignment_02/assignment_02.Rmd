---
title: "Comparing Cluster Methods"
author: "Ana Martins"
date: "2023-01-01"
output: html_document
---

```{r}
library(tidyverse)
library(patchwork)
library(mclust)
library(keras)

set.seed(42)
```


```{r}
cerv_cancer <- read_csv("raw_data/sobar-72.csv")
```

The chosen dataset is called "Cervical Cancer Behavior Risk Data Set" and it displays certain behaviours that a person has and if that person does or does not have cervical cancer. It was taken from [here](https://archive.ics.uci.edu/ml/datasets/Cervical+Cancer+Behavior+Risk), but the downloaded data seems to not exactly match the description of the website so we will make our own interpretations based on the variable names on the downloaded data.

The dataset has 20 arguments, with their names being pretty self-explanatory, namely `behaviour_sexualRisk`, `behaviour_eating`, `behaviour_personalHygine`, `intention_aggregation`, `intention_commitment`, `attitude_consistency`, `attitude_spontaneity`, `norm_significantPerson`, `norm_fulfillment`, `perception_vulnerability`, `perception_severity`, `motivation_strength`, `motivation_willingness`, `socialSupport_emotionality`, `socialSupport_appreciation`, `socialSupport_instrumental`, `empowerment_knowledge`, `empowerment_abilities`, `empowerment_desires`, `ca_cervix`. This last one is the target variable.

Our dataset actually came pretty tidy already, the only thing we need to do for it to be perfect is set the target variable to be a factor. 

```{r}
cerv_cancer <-
  cerv_cancer %>% 
  mutate(ca_cervix = as.factor(ca_cervix))
```

Let us check if our dataset has class imbalance.

```{r}
summary(cerv_cancer$ca_cervix)
```

Yes, we have many more negative results than positive results, so we are looking for uneven clusters. Furthermore, baseline accuracy is 70,83%.

Because we are lucky enough to have a supervised data set, let us divide it into training, where we will take out the target variable to do unsupervised clustering, and testing, where the target variable will be left so we can check the model's accuracy afterwards.

```{r}
split <- sample(c(0, 1), nrow(cerv_cancer), replace = TRUE, prob = c(0.80, 0.20))

cerv_cancer <-
  cerv_cancer %>% 
  mutate(split = split)

cerv_cancer_train <-
  cerv_cancer %>% 
  filter(split == 0) %>% 
  select(-split)

# we store this for later since we will need it
ca_cervix_train <- cerv_cancer_train$ca_cervix

cerv_cancer_train <-
  cerv_cancer_train %>% 
  select(-ca_cervix)

cerv_cancer_test <-
  cerv_cancer %>% 
  filter(split == 1) %>% 
  select(-split)
```

With that done, we move on to a harder task: deciding which columns to use for clustering. From a starting point, all of the 19 variables "are the same", meaning, they are all numerical variables so we cannot decide anything before checking correlation or the variables' distributions.

Let us start by trying to find the single variable that looks best for clustering. According to our analysis of class imbalance, we are looking for data that distributes into two non-equal clusters.

```{r}
ggplot(cerv_cancer, aes(behavior_sexualRisk)) + geom_density() + theme_minimal() +
  ggplot(cerv_cancer, aes(behavior_eating)) + geom_density() + theme_minimal() +
  ggplot(cerv_cancer, aes(behavior_personalHygine)) + geom_density() + theme_minimal() +
  ggplot(cerv_cancer, aes(intention_aggregation)) + geom_density() + theme_minimal()

ggplot(cerv_cancer, aes(intention_commitment)) + geom_density() + theme_minimal() +
  ggplot(cerv_cancer, aes(attitude_consistency)) + geom_density() + theme_minimal() +
  ggplot(cerv_cancer, aes(attitude_spontaneity)) + geom_density() + theme_minimal() +
  ggplot(cerv_cancer, aes(norm_significantPerson)) + geom_density() + theme_minimal()

ggplot(cerv_cancer, aes(norm_fulfillment)) + geom_density() + theme_minimal() +
  ggplot(cerv_cancer, aes(perception_vulnerability)) + geom_density() + theme_minimal() +
  ggplot(cerv_cancer, aes(perception_severity)) + geom_density() + theme_minimal() +
  ggplot(cerv_cancer, aes(motivation_strength)) + geom_density() + theme_minimal()

ggplot(cerv_cancer, aes(motivation_willingness)) + geom_density() + theme_minimal() +
  ggplot(cerv_cancer, aes(socialSupport_emotionality)) + geom_density() + theme_minimal() +
  ggplot(cerv_cancer, aes(socialSupport_appreciation)) + geom_density() + theme_minimal() +
  ggplot(cerv_cancer, aes(socialSupport_instrumental)) + geom_density() + theme_minimal()

ggplot(cerv_cancer, aes(empowerment_knowledge)) + geom_density() + theme_minimal() +
  ggplot(cerv_cancer, aes(empowerment_abilities)) + geom_density() + theme_minimal() +
  ggplot(cerv_cancer, aes(empowerment_desires)) + geom_density() + theme_minimal()
```

The variables that seem to have a good separation are: `norm_significantPerson`, `norm_fulfillment`, `perception_severity`, `socialSupport_emotionality`, `socialSupport_appreciation`, `socialSupport_instrumental`, `empowerment_knowledge` and `empowerment_desires`. Let us then try model-based clustering for all  of them, and pick the models with the highest BICs. We pick variable or equal variances depending on what performs best for each variable.

Considering cancer is such a complex thing to predict, we can already propose that this will not be the best solution, since we will try to predict it taking only one factor into account, but it is a good place to start, to know what we are working with.

```{r}
model_fitting_V <- function(x) {
x_fit <- Mclust(x, G = 2, modelNames = "V")
print(x_fit$BIC)

x_fit$classification
}

model_fitting_E <- function(x) {
x_fit <- Mclust(x, G = 2, modelNames = "E")
print(x_fit$BIC)

x_fit$classification
}
```


```{r}
norm_significantPerson_class <- model_fitting_V(cerv_cancer$norm_significantPerson)
```

```{r}
norm_fulfillment_class <- model_fitting_E(cerv_cancer$norm_fulfillment)
```

```{r}
perception_severity_class <- model_fitting_E(cerv_cancer$perception_severity)
```

```{r}
socialSupport_emotionality_class <- model_fitting_E(cerv_cancer$socialSupport_emotionality)
```

```{r}
socialSupport_appreciation_class <- model_fitting_V(cerv_cancer$socialSupport_appreciation)
```

```{r}
socialSupport_instrumental_class <- model_fitting_E(cerv_cancer$socialSupport_instrumental)
```

```{r}
empowerment_knowledge_class <- model_fitting_E(cerv_cancer$empowerment_knowledge)
```

```{r}
empowerment_desires_class <- model_fitting_V(cerv_cancer$empowerment_desires)
```

The highest BICs are for the following variables: `norm_significantPerson`, `perception_severity` and `socialSupport_appreciation`. The other ones have BICs that are quite far away from these and all close to each other, so these seem like good choices.

Since we are lucky enough to have the results in our dataset, let's compare. Let us build confusion matrixes for each model and get the accuracy. Let us also plot the clustering distribution and the real distribution to see if the model followed the right pattern.

```{r}
cerv_cancer <-
  cerv_cancer %>%
  mutate(
    nsP = as.factor(norm_significantPerson_class),
    sSa = as.factor(socialSupport_appreciation_class),
    ps = as.factor(perception_severity_class)
  )
```

```{r}
results_mbc <- function(class, x) { # function to produce the confusion matrix and plots
  
  #making the table
  cf <-
    table(pred = ifelse(class == 1, 0, 1),
          cerv_cancer$ca_cervix)
  
  if (sum(diag(cf)) < (cf[1, 2] + cf[2, 1])) {
    cf <-
      table(pred = ifelse(class == 1, 1, 0),
            cerv_cancer$ca_cervix)
    
    cerv_cancer <-
      cerv_cancer %>%
      mutate(class = as.factor(ifelse(class == 1, 2, 1)))
  }
  
  print(cf)
  
  accuracy <-
    sum(diag(cf)) / sum(cf) * 100
  
  print(accuracy)
  
  cerv_cancer %>%
    ggplot(aes(x = x, y = ca_cervix, color = ca_cervix)) +
    geom_jitter() +
    cerv_cancer %>%
    ggplot(aes(x = x, y = class, color = class)) +
    geom_jitter()
  
}
```


```{r}
results_mbc(cerv_cancer$nsP, cerv_cancer$norm_significantPerson)
```

```{r}
results_mbc(cerv_cancer$sSa, cerv_cancer$socialSupport_appreciation)
```

```{r}
results_mbc(cerv_cancer$ps, cerv_cancer$perception_severity)
```

That is what happens when you try to predict cancer with only one variable... The clustering models did find a good separation, but they are mostly wrong in their classification. You can even see that in most of them the separation is basically at the middle of their x-scale. The two first model are even below baseline accuracy. The model using only `perception_severity`, even though it had the larget BIC of the three, has teh best accuracy, just above baseline.

It seems that even with a high BIC, "blind" model-based clustering for such a complex issue will not be able to solve it.

A first instinct is to try and do some PCA, but PCA is known to not be very good at supervised analysis, so let us try and do some correlation analysis between the target variable and the other variables.

```{r}
corr <- vector()

for (i in c(1:19)) {
  corr[i] <- cor(as.numeric(cerv_cancer$ca_cervix),
                 cerv_cancer[i])
}

ord_corr <- sort(abs(corr))

plot(ord_corr)

ord_corr
corr
```

There seem to be 4 "breaks", where you can tell there is a big difference in correlation. So let us try to do k-means clustering with 5, 10, 14, 16, and all 19 variables. Let us calculate the accuracy and plot each cluster with the x axis being `perception_severity`, which had the best performance before, so the comparison is fair.

```{r}
results_kmeans <- function(data) {
  kmeans <- kmeans(data, 2)$cluster
  
  cf <-
    table(pred = ifelse(kmeans == 1, 0, 1), cerv_cancer$ca_cervix)
  
  if (sum(diag(cf)) < (cf[1, 2] + cf[2, 1])) {
    cf <- table(pred = ifelse(kmeans == 1, 1, 0), cerv_cancer$ca_cervix)
  }
  
  accuracy <- sum(diag(cf)) / sum(cf) * 100
  print(accuracy)
  
  cerv_cancer %>%
    ggplot(aes(x = perception_severity, y = ca_cervix, color = ca_cervix)) +
    geom_jitter() +
    cerv_cancer %>%
    ggplot(aes(x = perception_severity, y = kmeans, color = as.factor(kmeans))) +
    geom_jitter()
}
```


```{r}
cerv_cancer_5 <-
  cerv_cancer %>% 
  select(empowerment_abilities, perception_severity, empowerment_knowledge, motivation_strength, empowerment_desires)

results_kmeans(cerv_cancer_5)
```

```{r}
cerv_cancer_10 <-
  cerv_cancer %>% 
  select(empowerment_abilities, perception_severity, empowerment_knowledge, motivation_strength, empowerment_desires, motivation_willingness, norm_fulfillment, perception_vulnerability, socialSupport_emotionality, behavior_personalHygine)

results_kmeans(cerv_cancer_10)
```

```{r}
cerv_cancer_14 <-
  cerv_cancer %>% 
  select(empowerment_abilities, perception_severity, empowerment_knowledge, motivation_strength, empowerment_desires, motivation_willingness, norm_fulfillment, perception_vulnerability, socialSupport_emotionality, behavior_personalHygine, behavior_sexualRisk, socialSupport_appreciation, intention_aggregation, norm_significantPerson)

results_kmeans(cerv_cancer_14)
```

```{r}
cerv_cancer_16 <-
  cerv_cancer %>% 
  select(empowerment_abilities, perception_severity, empowerment_knowledge, motivation_strength, empowerment_desires, motivation_willingness, norm_fulfillment, perception_vulnerability, socialSupport_emotionality, behavior_personalHygine, behavior_sexualRisk, socialSupport_appreciation, intention_aggregation, norm_significantPerson, behavior_eating, intention_commitment)

results_kmeans(cerv_cancer_16)
```

```{r}
cerv_cancer_19 <-
  cerv_cancer %>% 
  select(-ca_cervix)

results_kmeans(cerv_cancer_19)
```

After running each a few times, we find that the model with 10 variables is pretty unstable, fluctuating between 65 to 80% accuracy. All of the other models also variate somewhat, but not as significantly, except for the model with 5 variables, which always stays the same.

Apart from that, taking the largest accuracy for each. they have all about the same accuracy, with the models based on 10, 14 and 16 features having slightly higher accuracy. Since we get almost the same result with only 5 variables, let us take this option as the best one and avoid overfitting altogether.

We can also see from the plots that it is not simply a split down the middle of the x axis, like in the model-based clustering, even thought `perception_severity` is one of the variables used for fitting.

The K-means models are much better than the model-based clustering models with only one variable.

However, it is still safe to say that clustering is not a good way to predict cancer, since the problem is simply not that linear. I would even go as far as saying that unsupervised learning altogether is not a good method of trying to predict complex problems, like cancer.

With that being said, let us actually try and do better, by comparing it to a supervised, non-linear, model, like a Neural Network. The perceptron is the simples Neural Network you can create, being simply one neuron. Let us see how it compares.

```{r}
cerv_cancer <- read_csv("raw_data/sobar-72.csv")
```


```{r}
perceptron <-
  keras_model_sequential(input_shape = 19) %>%
  layer_dense(2, activation = "softmax")
```

