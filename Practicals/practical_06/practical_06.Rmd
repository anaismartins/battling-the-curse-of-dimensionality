---
title: "Practical 6"
author: "Ana Martins"
date: "2022-12-16"
output: html_document
---

## 1 Introduction

```{r}
library(mclust)
library(tidyverse)
library(patchwork)
```

```{r}
df <- as_tibble(banknote)
```

## 2 Data exploration

**1. Read the help file of the banknote data set to understand what it’s all about.**

```{r}
?banknote
```

**2. Create a scatter plot of the left (x-axis) and right (y-axis) measurements on the data set. Map the Status column to colour. Jitter the points to avoid overplotting. Are the classes easy to distinguish based on these features?**

```{r}
df %>% 
  ggplot(aes(Left, Right, color = Status)) +
  geom_jitter() +
  theme_minimal()
```

Yes, kind of.

The classes are not easy to distinguish: there is considerable overlap.

**3. From now on, we will assume that we don’t have the labels. Remove the Status column from the data set.**

```{r}
df <-
  df %>% 
  select(-Status)
```

**4. Create density plots for all columns in the data set. Which single feature is likely to be best for clustering?**

```{r}
ggplot(df, aes(Length)) + geom_density() + theme_minimal() +
  ggplot(df, aes(Left)) + geom_density() + theme_minimal() +
  ggplot(df, aes(Right)) + geom_density() + theme_minimal() +
  ggplot(df, aes(Bottom)) + geom_density() + theme_minimal() +
  ggplot(df, aes(Top)) + geom_density() + theme_minimal() +
  ggplot(df, aes(Diagonal)) + geom_density() + theme_minimal()
```

Diagonal.

```{r}
# alternative:
library(ggridges)
df %>% 
  mutate_all(scale) %>% 
  pivot_longer(everything(), names_to = "feature", values_to = "value") %>% 
  ggplot(aes(x = value, y = feature, fill = feature)) + 
  geom_density_ridges() +
  scale_fill_viridis_d(guide = "none") +
  theme_minimal()
```


## 3 Univariate model-based clustering

**5. Use Mclust to perform model-based clustering with 2 clusters on the feature you chose. Assume equal variances. Name the model object fit_E_2. What are the means and variances of the clusters?**

```{r}
data1 <-
  df %>% 
  select(Diagonal)

fit_E_2 <- Mclust(data1, G = 2, modelNames = "E")
summary(fit_E_2, parameters = "TRUE")
fit_E_2$parameters$mean
fit_E_2$parameters$variance$sigmasq
```

**6. Use the formula from the slides and the model’s log-likelihood (fit_E_2$loglik) to compute the BIC for this model. Compare it to the BIC stored in the model object (fit_E_2$bic). Explain how many parameters (m) you used and which parameters these are.**

```{r}
# The model has 4 parameters: 1 class probability pi, 2 means and 1 variance
BIC <- -2 * fit_E_2$loglik + fit_E_2$df * 4
BIC
fit_E_2$bic
```

Note: the BIC from the Mclust package is negative!

**7. Plot the model-implied density using the plot() function. Afterwards, add rug marks of the original data to the plot using the rug() function from the base graphics system.**

```{r}
fit_E_2_plot <- plot(fit_E_2, "density")
rug(jitter(df$Diagonal), side = 1)
```

**8. Use Mclust to perform model-based clustering with 2 clusters on this feature again, but now assume unequal variances. Name the model object fit_V_2. What are the means and variances of the clusters? Plot the density again and note the differences.**

```{r}
fit_V_2 <- Mclust(data1, G = 2, modelNames = "V")
summary(fit_V_2, parameters = "TRUE")
fit_E_2_plot <- plot(fit_V_2, "density")
rug(jitter(df$Diagonal), side = 1)
```

Now the two clusters don't have the same size.

The left cluster has larger variance now, because the data is more spread out.

**9. How many parameters does this model have? Name them.**

```{r}
fit_V_2$df
```

5 parameters:
1 class probability pi
2 means
2 variances

**10. According to the deviance, which model fits better?**

```{r}
-2 * fit_E_2$loglik
-2 * fit_V_2$loglik
```

The one with unequal variances (lower deviance)

**11. According to the BIC, which model is better?**

```{r}
fit_E_2$BIC
fit_V_2$BIC
```

The one wit unequal variances.

## 3 Lab exercises

### 3.1 Multivariate model-based clustering

**12. Use Mclust with all 6 features to perform clustering. Allow all model types (shapes), and from 1 to 9 potential clusters. What is the optimal model based on the BIC?**

```{r}
fit <- Mclust(df)
summary(fit)
```
Model VEE with 3 clusterss.

**13. How many mean parameters does this model have?**

```{r}
Mclust(df, 3, "VEE")$parameters$mean
```

3 clusters * 6 variables * 18 mean parameters

**14. Run a 2-component VVV model on this data. Create a matrix of bivariate contour (“density”) plots using the plot() function. Which features provide good component separation? Which do not?**

```{r}
fit_VVV_2 <- Mclust(df, 2, "VVV")
plot(fit_VVV_2, "density")
```

Diagonal-Top and Diagonal-Bottom separate the lasses very well!

**15. Create a scatter plot just like the first scatter plot in this tutorial, but map the estimated class assignments to the colour aesthetic. Map the uncertainty (part of the fitted model list) to the size aesthetic, such that larger points indicate more uncertain class assignments. Jitter the points to avoid overplotting. What do you notice about the uncertainty?**

```{r}
df <-
  df %>% 
  mutate(VVV_2 = fit_VVV_2$classification, VVV_2_uncertainty = as.numeric(fit_VVV_2$uncertainty))

df %>% 
  ggplot(aes(Left, Right, color = as.factor(VVV_2), size = VVV_2_uncertainty)) +
  geom_jitter() +
  theme_minimal()
```

There are a few values with great uncertainty but most of them are pretty small.

Only 3 points are slightly uncertain, and they are not around the border of the classes in these two dimensions. The other dimensions give enough information about those points near the border!

### 3.2 Challenge assignment: High-dimensional Gaussian Mixture modeling

**16. Install and load the package HDclassif. Read the introduction and section 4.2, parts “First results” and “PCA representation” from the associated paper here.**

```{r}
library("HDclassif")
```

**17. Run high-dimensional data clustering on the Crabs dataset using demo("hddc"). Choose the EM algorithm with random initialization with the AkBkQkDk model. Explain what happens in the plot window.**

For each iteration, the classes get more and more recognised, starting from random.